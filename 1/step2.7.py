import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
import joblib
import os
import glob

# --- âš™ï¸ è¨­å®š ---
DATA_DIR = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\GooglePython\EMG\emg_data_svm"
MODEL_FILE = "svm_onset_model_tuned.joblib"
SCALER_FILE = "scaler_onset_svm_tuned.joblib"

WINDOW_SIZE = 10
M_SAMPLES = 10
CHANNELS = [2, 3, 6, 7]
FEATURE_COLS = [f"CH{c}" for c in CHANNELS]

# â˜… Onsetå®šç¾©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
ONSET_POSITIVE_WINDOWS = 10
ONSET_SD_MULTIPLIER = 1.5  # ğŸŒŸ ãƒã‚¤ã‚ºé–¾å€¤ã‚’ç·©å’Œ (3.0 -> 1.5)

# SVMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (RBF + ä¸å‡è¡¡å¯¾ç­–)
SVM_C = 0.1
SVM_GAMMA = "scale"

LABEL_MAP = {
    "rest": 0,
    "radial_dev": 1,
    "ulnar_dev": 2,
}
REVERSE_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}


# --- ğŸ“Š ç‰¹å¾´é‡æŠ½å‡ºé–¢æ•° (å‹å¤‰æ›ã«ã‚ˆã‚‹å®‰å®šåŒ–) ---
def extract_features(data, window_size, M, feature_cols):
    """RMSã¨DeltaRMSã®8æ¬¡å…ƒç‰¹å¾´é‡ã‚’æŠ½å‡ºã™ã‚‹"""
    N_samples = len(data)
    features = []

    # ğŸš¨ ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ1: EMGãƒ‡ãƒ¼ã‚¿åˆ—ã‚’å¼·åˆ¶çš„ã«floatå‹ã«å¤‰æ›ã—ã€ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–
    data[feature_cols] = data[feature_cols].apply(pd.to_numeric, errors="coerce")

    # ğŸš¨ ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ2: NaNã‚’å«ã‚€è¡Œã‚’ãƒ‰ãƒ­ãƒƒãƒ—
    data.dropna(subset=feature_cols, inplace=True)
    N_samples = len(data)  # ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å†è¨ˆç®—

    start_index = M + window_size - 1

    if N_samples <= start_index:
        print("ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ãŒç‰¹å¾´é‡æŠ½å‡ºã®æœ€å°è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã›ã‚“ã€‚")
        return pd.DataFrame(), np.array([])

    for i in range(start_index, N_samples):
        window_full = data.iloc[i - window_size + 1 : i + 1]
        window_signal = window_full[feature_cols]

        rms = np.sqrt(np.mean(window_signal**2, axis=0))

        if i >= M + window_size:
            past_window_full = data.iloc[i - M - window_size + 1 : i - M + 1]
            past_window_signal = past_window_full[feature_cols]

            # å†…éƒ¨ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã‚„ã™ã‹ã£ãŸç®‡æ‰€
            rms_past = np.sqrt(np.mean(past_window_signal**2, axis=0))
            delta_rms = rms - rms_past
        else:
            delta_rms = np.zeros_like(rms)

        feature_vector = rms.tolist() + delta_rms.tolist()
        features.append(feature_vector)

    labels_series = data["Label"].iloc[start_index:].values
    return (
        pd.DataFrame(
            features,
            columns=[f"{t}_{c}" for t in ["RMS", "DeltaRMS"] for c in feature_cols],
        ),
        labels_series,
    )


# --- ğŸ’¾ è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é–¢æ•° (ä»¥å‰ã¨åŒä¸€) ---
def load_data_from_directory(data_dir, channels, label_map):
    all_emg_data = []
    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
    if not csv_files:
        print(f"ã‚¨ãƒ©ãƒ¼: {data_dir} å†…ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    print(f"âœ… {len(csv_files)}å€‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")

    for file_path in csv_files:
        try:
            df = pd.read_csv(file_path)
            channel_cols = [f"CH{c}" for c in channels]
            required_cols = channel_cols + ["Label"]
            if not all(col in df.columns for col in required_cols):
                continue

            df_selected = df[required_cols].copy()
            df_selected["Label"] = df_selected["Label"].map(label_map)
            df_selected.dropna(subset=["Label"], inplace=True)
            df_selected["Label"] = df_selected["Label"].astype(int)
            all_emg_data.append(df_selected)
        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    if not all_emg_data:
        print("ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return None
    return pd.concat(all_emg_data, ignore_index=True)


# --- ğŸŒŸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†ãƒ©ãƒ™ãƒªãƒ³ã‚°é–¢æ•° (Onsetç‰¹åŒ–) ---
def relabel_for_onset(df, window_size, M, sd_multiplier, feature_cols):

    # 1. ç‰¹å¾´é‡ã®æŠ½å‡º (ã‚¨ãƒ©ãƒ¼å‡¦ç†ãŒå¼·åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã“ã“ã§å®‰å®šã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…)
    X_raw, y_raw = extract_features(df, window_size, M, feature_cols)

    if len(X_raw) == 0:
        print("ã‚¨ãƒ©ãƒ¼: ç‰¹å¾´é‡æŠ½å‡ºå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚")
        return pd.DataFrame(), np.array([])

    X_raw["Label"] = y_raw

    # 2. å®‰é™æ™‚ã®ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã‚’è¨ˆç®—
    rest_data = X_raw[X_raw["Label"] == LABEL_MAP["rest"]]
    if len(rest_data) == 0:
        print("ã‚¨ãƒ©ãƒ¼: Restãƒ©ãƒ™ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return X_raw.drop(columns=["Label"]), y_raw

    rest_rms_cols = [f"RMS_CH{c}" for c in CHANNELS]

    M_rms = rest_data[rest_rms_cols].mean().mean()
    SD_rms = rest_data[rest_rms_cols].values.std()

    T_noise = M_rms + sd_multiplier * SD_rms
    print(
        f"ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«é–¾å€¤ (T_noise): {T_noise:.4f} (å¹³å‡:{M_rms:.4f}, SD:{SD_rms:.4f})"
    )

    new_labels = np.full(len(X_raw), LABEL_MAP["rest"], dtype=int)

    # 3. å‹•ä½œã‚»ãƒƒã‚·ãƒ§ãƒ³ã”ã¨ã®Onsetã‚’ç‰¹å®šã—ã€ãƒ©ãƒ™ãƒ«ã‚’æ›¸ãæ›ãˆã‚‹
    current_label = LABEL_MAP["rest"]
    onset_started = False

    for i in range(len(X_raw)):
        row = X_raw.iloc[i]
        label = row["Label"]
        current_rms_mean = row[rest_rms_cols].mean()

        # çŠ¶æ…‹å¤‰åŒ–ãƒã‚§ãƒƒã‚¯
        if label != current_label:
            current_label = label
            onset_started = False
            onset_window_count = 0

        # Onsetã®ç‰¹å®šã¨ãƒ©ãƒ™ãƒªãƒ³ã‚°
        if current_label != LABEL_MAP["rest"]:

            # (A) Onsetãƒˆãƒªã‚¬ãƒ¼ã®ç™ºè¦‹
            if not onset_started and current_rms_mean > T_noise:
                onset_started = True
                onset_window_count = 1
                new_labels[i] = current_label

            # (B) OnsetæœŸé–“ã®æŒç¶š
            elif onset_started and onset_window_count < ONSET_POSITIVE_WINDOWS:
                onset_window_count += 1
                new_labels[i] = current_label

            # (C) å‹•ä½œæŒç¶šæœŸé–“ (Sustain) ã¯Restã«æˆ»ã™
            elif onset_started and onset_window_count >= ONSET_POSITIVE_WINDOWS:
                new_labels[i] = LABEL_MAP["rest"]

    X_new = X_raw.drop(columns=["Label"])
    y_new = new_labels

    print(
        f"æ–°ã—ã„Positiveã‚µãƒ³ãƒ—ãƒ«æ•°: {np.sum(y_new != 0)} (å…¨ä½“ã® {np.sum(y_new != 0) / len(y_new) * 100:.2f}%)"
    )
    return X_new, y_new


# --- ğŸ§ª ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ (Onset SVM) ---
def relabel_and_train_onset_svm():
    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    combined_df = load_data_from_directory(DATA_DIR, CHANNELS, LABEL_MAP)
    if combined_df is None:
        return

    print(f"ç·ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•° (ç”Ÿ): {len(combined_df)}")

    # 2. Onsetç‰¹åŒ–ãƒ©ãƒ™ãƒªãƒ³ã‚°ã¨ç‰¹å¾´é‡æŠ½å‡ºã®å®Ÿè¡Œ
    X_onset, y_onset = relabel_for_onset(
        combined_df, WINDOW_SIZE, M_SAMPLES, ONSET_SD_MULTIPLIER, FEATURE_COLS
    )

    if len(X_onset) == 0:
        return

    # 3. ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_onset.values, y_onset, test_size=0.2, random_state=42, stratify=y_onset
    )
    print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {len(X_train)}, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(X_test)}")

    # 4. ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 5. SVMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    print(
        f"\n--- SVM Onsetãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹ (C: {SVM_C}, Gamma: {SVM_GAMMA}, Weighted: Balanced) ---"
    )

    svm_model = SVC(
        kernel="rbf",
        C=SVM_C,
        gamma=SVM_GAMMA,
        class_weight="balanced",  # ğŸŒŸ ãƒ‡ãƒ¼ã‚¿ä¸å‡è¡¡å¯¾ç­–
        random_state=42,
    )
    svm_model.fit(X_train_scaled, y_train)

    # 6. è©•ä¾¡
    y_pred = svm_model.predict(X_test_scaled)

    print("\n--- SVM Onset Classification Report ---")
    target_names = [
        name for name, val in sorted(LABEL_MAP.items(), key=lambda item: item[1])
    ]
    print(
        classification_report(
            y_test, y_pred, target_names=target_names, zero_division=0
        )
    )

    # 7. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    joblib.dump(svm_model, MODEL_FILE)
    joblib.dump(scaler, SCALER_FILE)
    print(f"\n--- Model and Scaler saved to {MODEL_FILE} and {SCALER_FILE} ---")


if __name__ == "__main__":
    relabel_and_train_onset_svm()
