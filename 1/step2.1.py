import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
import joblib
import os
import glob

# --- è¨­å®š ---
# âš ï¸ ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
DATA_DIR = "./emg_data_svm/"
MODEL_FILE = "svm_drum_model_multi.joblib"
SCALER_FILE = "scaler_data_multi.joblib"

WINDOW_SIZE = 5
M_SAMPLES = 10

# EMGãƒãƒ£ãƒãƒ«ã®é¸æŠ (æ’“å±ˆ: 2, 3; å°ºå±ˆ: 6, 7)
CHANNELS = [2, 3, 6, 7]

# ãƒ©ãƒ™ãƒ«å¤‰æ›ãƒãƒƒãƒ—
LABEL_MAP = {
    "rest": 0,
    "radial_dev": 1,  # æ’“å±ˆ
    "ulnar_dev": 2,  # å°ºå±ˆ
}


# --- ç‰¹å¾´é‡æŠ½å‡ºé–¢æ•° (å¤‰æ›´ãªã—) ---
def extract_features(data, window_size, M, feature_cols):
    """
    data: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ  (CHxåˆ—ã¨Labelåˆ—ã‚’å«ã‚€)
    window_size: RMSè¨ˆç®—çª“å¹…
    M: å·®åˆ†è¨ˆç®—ã®é–“éš” (Î”R_kã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«å¿…è¦ãªéå»ãƒ‡ãƒ¼ã‚¿ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ)
    feature_cols: è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ä¿¡å·åˆ—ã®ãƒªã‚¹ãƒˆ (ä¾‹: ['CH1', 'CH5'])
    """
    N_samples = len(data)
    features = []

    start_index = M + window_size - 1

    for i in range(start_index, N_samples):
        # ç¾åœ¨ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        current_window_signal = data.iloc[i - window_size + 1 : i + 1][feature_cols]

        # éå»ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        past_window_signal = data.iloc[i - window_size + 1 - M : i + 1 - M][
            feature_cols
        ]

        # 1. çµ¶å¯¾RMS (R_k)
        rms = np.sqrt(np.mean(current_window_signal**2, axis=0))
        feature_vector = rms.tolist()

        # 2. å·®åˆ†RMS (Î”R_k)
        rms_past = np.sqrt(np.mean(past_window_signal**2, axis=0))
        delta_rms = rms - rms_past
        feature_vector.extend(delta_rms.tolist())

        features.append(feature_vector)

    # ãƒ©ãƒ™ãƒ«ã¯ã€ç‰¹å¾´é‡ãŒè¨ˆç®—ã•ã‚ŒãŸæ™‚ç‚¹ (ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®æœ«å°¾) ã®ã‚‚ã®ã‚’ä½¿ç”¨
    labels_series = data["Label"].iloc[start_index:].values

    return np.array(features), labels_series


# --- è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é–¢æ•° (å¤‰æ›´ãªã—) ---
def load_data_from_directory(data_dir, channels, label_map):
    all_emg_data = []
    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))

    if not csv_files:
        print(f"ã‚¨ãƒ©ãƒ¼: {data_dir} å†…ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    print(f"âœ… {len(csv_files)}å€‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")

    for file_path in csv_files:
        try:
            df = pd.read_csv(file_path)
            channel_cols = [f"CH{c}" for c in channels]
            required_cols = channel_cols + ["Label"]

            if not all(col in df.columns for col in required_cols):
                print(
                    f"è­¦å‘Š: {file_path} ã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
                )
                continue

            df_selected = df[required_cols].copy()
            df_selected["Label"] = df_selected["Label"].map(label_map)
            df_selected.dropna(subset=["Label"], inplace=True)
            df_selected["Label"] = df_selected["Label"].astype(int)
            all_emg_data.append(df_selected)

        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            continue

    if not all_emg_data:
        print("ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    return pd.concat(all_emg_data, ignore_index=True)


# --- ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ (ä¿®æ­£ç®‡æ‰€ã‚ã‚Š) ---
def train_model():
    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    combined_df = load_data_from_directory(DATA_DIR, CHANNELS, LABEL_MAP)

    if combined_df is None:
        return

    print(f"ç·ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(combined_df)}")

    # 2. ç‰¹å¾´é‡æŠ½å‡º
    emg_cols = [f"CH{c}" for c in CHANNELS]
    X, y = extract_features(combined_df, WINDOW_SIZE, M_SAMPLES, feature_cols=emg_cols)

    if len(X) == 0:
        print(
            "ã‚¨ãƒ©ãƒ¼: ç‰¹å¾´é‡æŠ½å‡ºå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚„M_SAMPLESã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        )
        return

    feature_names = [f"RMS_{col}" for col in emg_cols] + [
        f"DeltaRMS_{col}" for col in emg_cols
    ]

    print(f"ç‰¹å¾´é‡æŠ½å‡ºå¾Œã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}")
    print(f"ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡ ({len(feature_names)}æ¬¡å…ƒ): {', '.join(feature_names)}")

    # 2.5. ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®ç¢ºèª
    print("\n--- ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ ---")
    label_counts = pd.Series(y).value_counts().sort_index()
    label_map_rev = {v: k for k, v in LABEL_MAP.items()}
    for label_val, count in label_counts.items():
        print(f"  {label_map_rev.get(label_val, 'ä¸æ˜')}: {count} ã‚µãƒ³ãƒ—ãƒ«")

    # 3. ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # 4. å­¦ç¿’/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"\nå­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {len(X_train)}, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(X_test)}")

    # 5. SVMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    # ğŸŒŸ å¤‰æ›´ç‚¹: kernel="rbf", C=1.0 ã«è¨­å®š
    print("\n--- SVMãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹ (Kernel: RBF, C: 1.0) ---")
    svm_model = SVC(kernel="rbf", C=1.0, random_state=42)
    svm_model.fit(X_train, y_train)

    # 6. è©•ä¾¡
    y_pred = svm_model.predict(X_test)

    print("\n--- Classification Report ---")
    target_names = [
        name for name, val in sorted(LABEL_MAP.items(), key=lambda item: item[1])
    ]
    print(
        classification_report(
            y_test, y_pred, target_names=target_names, zero_division=0
        )
    )

    # 7. ãƒ¢ãƒ‡ãƒ«ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¿å­˜
    joblib.dump(svm_model, MODEL_FILE)
    joblib.dump(scaler, SCALER_FILE)
    print(f"\n--- Model and Scaler saved to **{MODEL_FILE}** and **{SCALER_FILE}** ---")


if __name__ == "__main__":
    train_model()
