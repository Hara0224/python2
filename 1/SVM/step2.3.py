import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler
import joblib
import os
import glob

# --- è¨­å®š ---
DATA_DIR = "./emg_data_svm/"
MODEL_FILE = "svm_drum_model_multi_zc.joblib"  # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´
SCALER_FILE = "scaler_data_multi_zc.joblib"  # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰æ›´

# ğŸŒŸ å¤‰æ›´ç‚¹: RMSè¨ˆç®—ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’10ã«å¤‰æ›´
WINDOW_SIZE = 10  # RMSè¨ˆç®—ã®ãŸã‚ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
M_SAMPLES = 10  # å·®åˆ†ç‰¹å¾´é‡ã®æ¯”è¼ƒå¯¾è±¡ã‚µãƒ³ãƒ—ãƒ«é–“éš”

# EMGãƒãƒ£ãƒãƒ«ã®é¸æŠ
CHANNELS = [2, 6, 7]

# ãƒ©ãƒ™ãƒ«å¤‰æ›ãƒãƒƒãƒ—
LABEL_MAP = {
    "rest": 0,
    "radial_dev": 1,  # æ’“å±ˆ
    "ulnar_dev": 2,  # å°ºå±ˆ
}

# â˜… RBFã‚«ãƒ¼ãƒãƒ«è¨­å®š
SVM_GAMMA = 0.1
SVM_C = 0.1
ZC_THRESHOLD = 0.001  # ZCè¨ˆç®—ç”¨ã®é–¾å€¤ (ãƒã‚¤ã‚ºå¯¾ç­–ã€‚å¿…è¦ã«å¿œã˜ã¦èª¿æ•´)


# --- ç‰¹å¾´é‡æŠ½å‡ºé–¢æ•° (ZCè¿½åŠ ç‰ˆ) ---
def extract_features(data, window_size, M, feature_cols, zc_threshold):
    """
    ZC (Zero Crossing) ç‰¹å¾´é‡ã‚’è¿½åŠ 
    """
    N_samples = len(data)
    features = []

    start_index = M + window_size - 1

    for i in range(start_index, N_samples):
        # ç¾åœ¨ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        current_window_signal = data.iloc[i - window_size + 1 : i + 1][feature_cols]

        # éå»ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        past_window_signal = data.iloc[i - window_size + 1 - M : i + 1 - M][
            feature_cols
        ]

        feature_vector = []

        # 1. çµ¶å¯¾RMS (R_k)
        rms = np.sqrt(np.mean(current_window_signal**2, axis=0))
        feature_vector.extend(rms.tolist())

        # 2. å·®åˆ†RMS (Î”R_k)
        rms_past = np.sqrt(np.mean(past_window_signal**2, axis=0))
        delta_rms = rms - rms_past
        feature_vector.extend(delta_rms.tolist())

        # 3. ZC (Zero Crossing) ã®è¨ˆç®— (æ–°è¦è¿½åŠ )
        # ä¿¡å·ãŒé–¾å€¤å¤–ã§ç¬¦å·ã‚’è·¨ã„ã å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        # numpy.diff(np.sign(signal)) ã¯ç¬¦å·ã®å¤‰åŒ–ã‚’æ‰ãˆã‚‹
        # çµ¶å¯¾å€¤ãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã ãŒã€
        # ã“ã“ã§ã¯ãƒã‚¤ã‚ºè€æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã€éš£æ¥è¦ç´ ã®ç©ã§è² ã«ãªã‚‹å›æ•°ï¼ˆç¬¦å·åè»¢ï¼‰ã‚’å˜ç´”ã«ã‚«ã‚¦ãƒ³ãƒˆ

        zc_counts = []
        for col in feature_cols:
            signal = current_window_signal[col].values

            # éš£æ¥è¦ç´ ã®ç©ãŒè²  ã‹ã¤ ã©ã¡ã‚‰ã‹ã®çµ¶å¯¾å€¤ãŒé–¾å€¤ã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            # ãƒã‚¤ã‚ºã‚’ç„¡è¦–ã™ã‚‹ãŸã‚ã«é–¾å€¤ã‚’ä½¿ç”¨ï¼ˆã“ã“ã§ã¯å˜ç´”åŒ–ã®ãŸã‚ã€é–¾å€¤ã¯ç„¡è¦–ã—ç¬¦å·åè»¢ã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼‰
            # ã‚ˆã‚Šãƒ­ãƒã‚¹ãƒˆãªZCè¨ˆç®—:
            zc = np.sum(
                (signal[:-1] * signal[1:] < 0)
                & (
                    (np.abs(signal[:-1]) > zc_threshold)
                    | (np.abs(signal[1:]) > zc_threshold)
                )
            )
            zc_counts.append(zc)

        feature_vector.extend(zc_counts)
        features.append(feature_vector)

    # ãƒ©ãƒ™ãƒ«ã¯ã€ç‰¹å¾´é‡ãŒè¨ˆç®—ã•ã‚ŒãŸæ™‚ç‚¹ (ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®æœ«å°¾) ã®ã‚‚ã®ã‚’ä½¿ç”¨
    labels_series = data["Label"].iloc[start_index:].values

    return np.array(features), labels_series


# --- è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰é–¢æ•° (å¤‰æ›´ãªã—) ---
def load_data_from_directory(data_dir, channels, label_map):
    all_emg_data = []
    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))

    if not csv_files:
        print(f"ã‚¨ãƒ©ãƒ¼: {data_dir} å†…ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    print(f"âœ… {len(csv_files)}å€‹ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")

    for file_path in csv_files:
        try:
            df = pd.read_csv(file_path)
            channel_cols = [f"CH{c}" for c in channels]
            required_cols = channel_cols + ["Label"]

            if not all(col in df.columns for col in required_cols):
                print(
                    f"è­¦å‘Š: {file_path} ã«å¿…è¦ãªã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
                )
                continue

            df_selected = df[required_cols].copy()
            df_selected["Label"] = df_selected["Label"].map(label_map)
            df_selected.dropna(subset=["Label"], inplace=True)
            df_selected["Label"] = df_selected["Label"].astype(int)
            all_emg_data.append(df_selected)

        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            continue

    if not all_emg_data:
        print("ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    return pd.concat(all_emg_data, ignore_index=True)


# --- ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ (C=0.1, ZCè¿½åŠ ã«è¨­å®š) ---
def train_model():
    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    combined_df = load_data_from_directory(DATA_DIR, CHANNELS, LABEL_MAP)
    if combined_df is None:
        return

    print(f"ç·ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(combined_df)}")

    # 2. ç‰¹å¾´é‡æŠ½å‡º
    emg_cols = [f"CH{c}" for c in CHANNELS]
    # ZC_THRESHOLDã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™
    X, y = extract_features(
        combined_df,
        WINDOW_SIZE,
        M_SAMPLES,
        feature_cols=emg_cols,
        zc_threshold=ZC_THRESHOLD,
    )
    if len(X) == 0:
        return

    # ç‰¹å¾´é‡ã®åå‰ã‚’ç”Ÿæˆ (RMS, DeltaRMS, ZC)
    feature_names = (
        [f"RMS_{col}" for col in emg_cols]
        + [f"DeltaRMS_{col}" for col in emg_cols]
        + [f"ZC_{col}" for col in emg_cols]
    )

    print(f"ç‰¹å¾´é‡æŠ½å‡ºå¾Œã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X)}")
    print(f"ç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡ ({len(feature_names)}æ¬¡å…ƒ): {', '.join(feature_names)}")

    # 2.5. ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®ç¢ºèª
    print("\n--- ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ ---")
    label_counts = pd.Series(y).value_counts().sort_index()
    label_map_rev = {v: k for k, v in LABEL_MAP.items()}
    for label_val, count in label_counts.items():
        print(f"  {label_map_rev.get(label_val, 'ä¸æ˜')}: {count} ã‚µãƒ³ãƒ—ãƒ«")

    # 3. ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # 4. å­¦ç¿’/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"\nå­¦ç¿’ãƒ‡ãƒ¼ã‚¿æ•°: {len(X_train)}, ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(X_test)}")

    # 5. SVMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
    print(f"\n--- SVMãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹ (Kernel: RBF, C: {SVM_C}, Gamma: {SVM_GAMMA}) ---")
    svm_model = SVC(kernel="rbf", C=SVM_C, gamma=SVM_GAMMA, random_state=42)
    svm_model.fit(X_train, y_train)

    # 6. è©•ä¾¡
    y_pred = svm_model.predict(X_test)

    print("\n--- Classification Report ---")
    target_names = [
        name for name, val in sorted(LABEL_MAP.items(), key=lambda item: item[1])
    ]
    print(
        classification_report(
            y_test, y_pred, target_names=target_names, zero_division=0
        )
    )

    # 7. ãƒ¢ãƒ‡ãƒ«ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®ä¿å­˜
    joblib.dump(svm_model, MODEL_FILE)
    joblib.dump(scaler, SCALER_FILE)
    print(f"\n--- Model and Scaler saved to **{MODEL_FILE}** and **{SCALER_FILE}** ---")


if __name__ == "__main__":
    train_model()
